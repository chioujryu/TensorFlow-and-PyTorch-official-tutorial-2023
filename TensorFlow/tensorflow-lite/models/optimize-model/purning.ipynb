{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Pruning for on-device inference w/ XNNPACK](#toc1_)    \n",
    "  - 1.1. [Setup 设置](#toc1_1_)    \n",
    "  - 1.2. [Build and train the dense model 构建和训练密集模型](#toc1_2_)    \n",
    "  - 1.3. [Build the sparse model 构建稀疏模型](#toc1_3_)    \n",
    "    - 1.3.1. [Fine-tune the sparse model 微調稀疏模型](#toc1_3_1_)    \n",
    "  - 1.4. [Model conversion and benchmarking 模型轉換和基準測試](#toc1_4_)    \n",
    "  - 1.5. [查看 Model 大小](#toc1_5_)    \n",
    "  - 1.6. [Conclusion 結論](#toc1_6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1_'></a>[Pruning for on-device inference w/ XNNPACK](#toc0_)\n",
    "\n",
    "Welcome to the guide on Keras weights pruning for improving latency of on-device inference via [XNNPACK](https://github.com/google/XNNPACK).\n",
    "\n",
    "欢迎阅读Keras权重修剪指南，以通过XNNPACK改善设备上推理的延迟。\n",
    "\n",
    "<br>\n",
    "\n",
    "This guide presents the usage of the newly introduced `tfmot.sparsity.keras.PruningPolicy` API and demonstrates how it could be used for accelerating mostly convolutional models on modern CPUs using [XNNPACK Sparse inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#sparse-inference).\n",
    "\n",
    "本指南介绍了新引入的API的用法 `tfmot.sparsity.keras.PruningPolicy` ，并演示了如何使用XNNPACK稀疏推理在现代CPU上加速主要的卷积模型。\n",
    "\n",
    "<br>\n",
    "\n",
    "The guide covers the following steps of the model creation process:\n",
    "\n",
    "本指南涵盖了模型创建过程的以下步骤：\n",
    "\n",
    "* Build and train the dense baseline 构建并训练密集基线\n",
    "* Fine-tune model with pruning 使用修剪微调模型\n",
    "* Convert to TFLite 转换为TFLite\n",
    "* On-device benchmark 设备上基准测试\n",
    "\n",
    "<br>\n",
    "\n",
    "The guide doesn't cover the best practices for the fine-tuning with pruning. For more detailed information on this topic, please check out our [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md).\n",
    "\n",
    "本指南没有介绍使用修剪进行微调的最佳实践。有关此主题的更多详细信息，请查看我们的[综合指南](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md)。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. <a id='toc1_1_'></a>[Setup 设置](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q tensorflow\n",
    "# pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. <a id='toc1_2_'></a>[Build and train the dense model 构建和训练密集模型](#toc0_)\n",
    "\n",
    "We build and train a simple baseline CNN for classification task on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.\n",
    "\n",
    "我们构建并训练了一个简单的基线CNN，用于[CIFAR10数据集](https://www.cs.toronto.edu/~kriz/cifar.html)的分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 10:02:48.128136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.128238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.141922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.142038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.142095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.142150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.142387: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-30 10:02:48.245055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.245156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.245220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.245270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.245319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.245370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.740509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.740635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.740695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.740750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.740800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.740867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46037 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2023-06-30 10:02:48.741165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 10:02:48.741224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46698 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:04:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 dataset.\n",
    "\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\"cifar10\",\n",
    "                                        split=['train[:90%]', 'train[90%:]', 'test'],\n",
    "                                        as_supervised=True,\n",
    "                                        with_info=True,\n",
    "                                        )\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "def normalize_img(img, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.image.convert_image_dtype(img, tf.float32), label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in batches of 128 images.\n",
    "\n",
    "batch_size = 128\n",
    "def prepare_dataset(ds, buffer_size=None):\n",
    "    ds = ds.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE) # 将ds数据集中的每个元素传递给normalize_img函数进行预处理。\n",
    "    ds = ds.cache() # cache方法将预处理后的数据集缓存到内存中，可以在需要多次迭代访问数据集时提高访问速度。\n",
    "    if buffer_size:\n",
    "        ds = ds.shuffle(buffer_size)\n",
    "    ds = ds.batch(batch_size) # ds.batch(batch_size)表示将ds数据集划分为多个批次，每个批次包含batch_size个元素。\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE) # 当模型训练时，prefetch方法会异步地从数据集中预取一定数量的元素，并将它们放入缓冲区中。\n",
    "    return ds\n",
    "\n",
    "ds_train = prepare_dataset(ds_train, ds_info.splits['train'].num_examples)\n",
    "ds_val = prepare_dataset(ds_val)\n",
    "ds_test = prepare_dataset(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dense baseline model.\n",
    "\n",
    "dense_model = keras.Sequential([keras.layers.InputLayer(input_shape=(32,32,3)),\n",
    "\n",
    "                                keras.layers.ZeroPadding2D(padding=1), # 使用ZeroPadding2D层，我们可以有效地扩展输入特征图的大小，而不会损失输入图像的信息。\n",
    "\n",
    "                                keras.layers.Conv2D(filters=8,kernel_size=(3, 3),strides=(2, 2),padding='valid'),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.ReLU(),\n",
    "\n",
    "                                keras.layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same'),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.ReLU(),\n",
    "\n",
    "                                keras.layers.Conv2D(filters=16, kernel_size=(1, 1)),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.ReLU(),\n",
    "\n",
    "                                keras.layers.ZeroPadding2D(padding=1),\n",
    "\n",
    "                                keras.layers.DepthwiseConv2D(kernel_size=(3, 3), strides=(2, 2), padding='valid'),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.ReLU(),\n",
    "\n",
    "                                keras.layers.Conv2D(filters=32, kernel_size=(1, 1)),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.ReLU(),\n",
    "\n",
    "                                keras.layers.GlobalAveragePooling2D(),\n",
    "                                keras.layers.Flatten(),\n",
    "                                keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the dense model for 10 epochs.\n",
    "dense_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 10:02:50.961762: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8600\n",
      "2023-06-30 10:02:51.303033: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-06-30 10:02:51.363886: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 - 3s - loss: 1.9814 - accuracy: 0.2736 - val_loss: 2.2209 - val_accuracy: 0.1674 - 3s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "352/352 - 1s - loss: 1.7085 - accuracy: 0.3657 - val_loss: 1.6562 - val_accuracy: 0.3934 - 935ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "352/352 - 1s - loss: 1.6018 - accuracy: 0.4130 - val_loss: 1.6059 - val_accuracy: 0.3984 - 814ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "352/352 - 1s - loss: 1.5480 - accuracy: 0.4352 - val_loss: 1.5534 - val_accuracy: 0.4264 - 795ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "352/352 - 1s - loss: 1.5107 - accuracy: 0.4527 - val_loss: 1.6707 - val_accuracy: 0.3994 - 785ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "352/352 - 1s - loss: 1.4785 - accuracy: 0.4693 - val_loss: 1.5060 - val_accuracy: 0.4508 - 771ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "352/352 - 1s - loss: 1.4525 - accuracy: 0.4775 - val_loss: 1.4310 - val_accuracy: 0.4810 - 747ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "352/352 - 1s - loss: 1.4259 - accuracy: 0.4860 - val_loss: 1.5433 - val_accuracy: 0.4500 - 916ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "352/352 - 1s - loss: 1.4054 - accuracy: 0.4929 - val_loss: 1.4364 - val_accuracy: 0.4686 - 742ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "352/352 - 1s - loss: 1.3899 - accuracy: 0.4998 - val_loss: 1.4695 - val_accuracy: 0.4734 - 792ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3fbc0e2d00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.fit(\n",
    "  ds_train,\n",
    "  epochs=10,\n",
    "  validation_data=ds_val,\n",
    "  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 2ms/step - loss: 1.4690 - accuracy: 0.4765\n",
      "dense_model_accuracy =  0.476500004529953\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the dense model\n",
    "_ , dense_model_accuracy = dense_model.evaluate(ds_test, verbose=1)\n",
    "print(\"dense_model_accuracy = \", dense_model_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. <a id='toc1_3_'></a>[Build the sparse model 构建稀疏模型](#toc0_)\n",
    "\n",
    "Using the instructions from the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md), we apply `tfmot.sparsity.keras.prune_low_magnitude` function with parameters that target on-device acceleration via pruning i.e. `tfmot.sparsity.keras.PruneForLatencyOnXNNPack` policy.\n",
    "\n",
    "使用来自[综合指南](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md)的说明，我们应用 `tfmot.sparsity.keras.prune_low_magnitude` 具有通过修剪以设备上加速为目标的参数的函数，即政策 `tfmot.sparsity.keras.PruneForLatencyOnXNNPack` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after after 5 epochs.\n",
    "end_epochs = 5\n",
    "\n",
    "num_iterations_per_epoch = len(ds_train)  # 幾個 batch\n",
    "end_step  = num_iterations_per_epoch  * end_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for pruning.\n",
    "pruning_params = {\n",
    "    'pruning_schedule':tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.25,\n",
    "                                                            final_sparsity=0.75,\n",
    "                                                            begin_step=0,\n",
    "                                                            end_step=end_step),\n",
    "    'pruning_policy':tfmot.sparsity.keras.PruneForLatencyOnXNNPack()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to apply pruning wrapper with pruning policy parameter.\n",
    "try:\n",
    "    model_for_pruning = prune_low_magnitude(dense_model, **pruning_params)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call `prune_low_magnitude` results in `ValueError` with the message `Could not find a GlobalAveragePooling2D layer with keepdims = True in all output branches`. The message indicates that the model isn't supported for pruning with policy [tfmot.sparsity.keras.PruneForLatencyOnXNNPack](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruneForLatencyOnXNNPack) and specifically the layer `GlobalAveragePooling2D` requires the parameter `keepdims = True`. Let's fix that and reapply `prune_low_magnitude` function.\n",
    "\n",
    "呼叫 prune_low_magnitude 結果 ValueError 為消息 Could not find a GlobalAveragePooling2D layer with keepdims = True in all output branches 。該消息指示模型不支持使用策略進行修剪 [tfmot.sparsity.keras.PruneForLatencyOnXNNPack](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruneForLatencyOnXNNPack) ，並且該層 GlobalAveragePooling2D 需要該參數 keepdims = True 。讓我們修復它並重新應用 prune_low_magnitude 函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dense_model = keras.Sequential([keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
    "                  \n",
    "                    keras.layers.ZeroPadding2D(padding=1),\n",
    "\n",
    "                    keras.layers.Conv2D(filters=8,kernel_size=(3, 3),strides=(2, 2),padding='valid'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.ReLU(),\n",
    "                    \n",
    "                    keras.layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.ReLU(),\n",
    "                    \n",
    "                    keras.layers.Conv2D(filters=16, kernel_size=(1, 1)),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.ReLU(),\n",
    "\n",
    "                    keras.layers.ZeroPadding2D(padding=1),\n",
    "\n",
    "                    keras.layers.DepthwiseConv2D(kernel_size=(3, 3), strides=(2, 2), padding='valid'),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.ReLU(),\n",
    "\n",
    "                    keras.layers.Conv2D(filters=32, kernel_size=(1, 1)),\n",
    "                    keras.layers.BatchNormalization(),\n",
    "                    keras.layers.ReLU(),\n",
    "                    \n",
    "                    keras.layers.GlobalAveragePooling2D(keepdims=True),\n",
    "                    keras.layers.Flatten(),\n",
    "                    keras.layers.Dense(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pretrained model for pruning instead of training from scratch.\n",
    "fixed_dense_model.set_weights(dense_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reapply pruning wrapper.\n",
    "model_for_pruning = prune_low_magnitude(fixed_dense_model, **pruning_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invocation of `prune_low_magnitude` has finished without any errors meaning that the model is fully supported for the [tfmot.sparsity.keras.PruneForLatencyOnXNNPack](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PruneForLatencyOnXNNPack) policy and can be accelerated using [XNNPACK Sparse inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#sparse-inference).\n",
    "\n",
    "的調用 `prune_low_magnitude` 已完成，沒有任何錯誤，這意味著模型完全支持 tfmot.sparsity.keras.PruneForLatencyOnXNNPack 策略，並且可以使用[XNNPACK稀疏推理](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#sparse-inference)來加速。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. <a id='toc1_3_1_'></a>[Fine-tune the sparse model 微調稀疏模型](#toc0_)\n",
    "Following the [pruning example](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras.md), we fine-tune the sparse model using the weights of the dense model. We start fine-tuning of the model with 25% sparsity (25% of the weights are set to zero) and end with 75% sparsity.\n",
    "\n",
    "在[修剪示例](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras.md)之後，我們使用密集模型的權重來微調稀疏模型。我們以25%的稀疏度（25%的權重設置為零）開始對模型進行微調，並以75%的稀疏度結束。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/352 [..............................] - ETA: 2:38 - loss: 1.3781 - accuracy: 0.5391WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0025s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 4s 9ms/step - loss: 1.3891 - accuracy: 0.4989 - val_loss: 1.4183 - val_accuracy: 0.4898\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.4123 - accuracy: 0.4945 - val_loss: 1.6689 - val_accuracy: 0.4026\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.4306 - accuracy: 0.4876 - val_loss: 1.5408 - val_accuracy: 0.4336\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 1.4283 - accuracy: 0.4866 - val_loss: 1.4723 - val_accuracy: 0.4690\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4255 - accuracy: 0.4860 - val_loss: 1.4527 - val_accuracy: 0.4802\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.4141 - accuracy: 0.4929 - val_loss: 1.4224 - val_accuracy: 0.4888\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.4084 - accuracy: 0.4950 - val_loss: 1.4456 - val_accuracy: 0.4788\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4021 - accuracy: 0.4980 - val_loss: 1.4314 - val_accuracy: 0.4842\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 1.3949 - accuracy: 0.5010 - val_loss: 1.4275 - val_accuracy: 0.4876\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.3908 - accuracy: 0.5018 - val_loss: 1.4903 - val_accuracy: 0.4570\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 1.3873 - accuracy: 0.5026 - val_loss: 1.3961 - val_accuracy: 0.5064\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 2s 6ms/step - loss: 1.3813 - accuracy: 0.5048 - val_loss: 1.4491 - val_accuracy: 0.4770\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.3801 - accuracy: 0.5060 - val_loss: 1.4598 - val_accuracy: 0.4606\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 3s 8ms/step - loss: 1.3761 - accuracy: 0.5070 - val_loss: 1.6702 - val_accuracy: 0.3796\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 3s 7ms/step - loss: 1.3732 - accuracy: 0.5098 - val_loss: 1.5376 - val_accuracy: 0.4646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ffc9aa0d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp() # Create temp dir 創建臨時目錄\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.fit(\n",
    "  ds_train,\n",
    "  epochs=15,\n",
    "  validation_data=ds_val,\n",
    "  callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs show the progression of sparsity on a per-layer basis.\n",
    "\n",
    "日誌顯示了稀疏性在每層基礎上的進展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense model test accuracy: 0.476500004529953\n",
      "Pruned model test accuracy: 0.44690001010894775\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the dense model.\n",
    "_, pruned_model_accuracy = model_for_pruning.evaluate(ds_test, verbose=0)\n",
    "\n",
    "print('Dense model test accuracy:', dense_model_accuracy)\n",
    "print('Pruned model test accuracy:', pruned_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #docs_infra: no_execute\n",
    "# %tensorboard --logdir={logdir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the fine-tuning with pruning, test accuracy demonstrates a modest improvement (43% to 44%) compared to the dense model. Let's compare on-device latency using [TFLite benchmark](https://www.tensorflow.org/lite/performance/measurement).\n",
    "\n",
    "在使用修剪進行微調之後，與密集模型相比，測試準確率顯示出適度的提高（43%至44%）。讓我們使用[TFLite基準測試](https://www.tensorflow.org/lite/performance/measurement)來比較設備上的延遲。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. <a id='toc1_4_'></a>[Model conversion and benchmarking 模型轉換和基準測試](#toc0_)\n",
    "\n",
    "To convert the pruned model into TFLite, we need replace the `PruneLowMagnitude` wrappers with original layers via the `strip_pruning` function. Also, since the weights of the pruned model (`model_for_pruning`) are mostly zeros, we may apply an optimization [tf.lite.Optimize.EXPERIMENTAL_SPARSITY](https://www.tensorflow.org/lite/api_docs/python/tf/lite/Optimize#EXPERIMENTAL_SPARSITY) to efficiently store the resulted TFLite model. This optimization flag is not required for the dense model.\n",
    "\n",
    "要將修剪後的模型轉換為TFLite，我們需要通過函數將 `PruneLowMagnitude` 包裝器替換為原始層 `strip_pruning` 。此外，由於修剪模型（`model_for_pruning`）的權重  大多為零，因此我們可以應用優化 [tf.lite.Optimize.EXPERIMENTAL_SPARSITY](https://www.tensorflow.org/lite/api_docs/python/tf/lite/Optimize#EXPERIMENTAL_SPARSITY) 來有效地存儲所得到的TFLite模型。密集模型不需要此優化標誌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmponl7c5ul/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmponl7c5ul/assets\n",
      "2023-06-30 11:11:52.734662: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-30 11:11:52.734681: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-30 11:11:52.734789: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmponl7c5ul\n",
      "2023-06-30 11:11:52.736576: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-30 11:11:52.736586: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmponl7c5ul\n",
      "2023-06-30 11:11:52.742202: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-30 11:11:52.790649: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmponl7c5ul\n",
      "2023-06-30 11:11:52.805829: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 71041 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(dense_model)\n",
    "dense_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpopi3mcs6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpopi3mcs6/assets\n",
      "2023-06-30 11:11:53.917989: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-30 11:11:53.918007: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-30 11:11:53.918114: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpopi3mcs6\n",
      "2023-06-30 11:11:53.919793: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-30 11:11:53.919804: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpopi3mcs6\n",
      "2023-06-30 11:11:53.924642: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-30 11:11:53.950014: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpopi3mcs6\n",
      "2023-06-30 11:11:53.961224: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 43110 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# 儲存 dense_tflite_model\n",
    "_, dense_tflite_file = tempfile.mkstemp('.tflite') # Create temp dir 創建臨時目錄\n",
    "with open(dense_tflite_file, 'wb') as f:\n",
    "  f.write(dense_tflite_model)\n",
    "\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "# 儲存 pruned_tflite_model\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the instructions of [TFLite Model Benchmarking Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark), we build the tool, upload it to the Android device together with dense and pruned TFLite models, and benchmark both models on the device.\n",
    "\n",
    "按照[TFLite模型基準測試工具](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)的說明，我們構建了該工具，將其與密集和修剪的TFLite模型一起上傳到Android設備，並在設備上對兩個模型進行基準測試。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: adb：命令找不到\n"
     ]
    }
   ],
   "source": [
    "!adb shell /data/local/tmp/benchmark_model \\\n",
    "    --graph=/data/local/tmp/dense_model.tflite \\\n",
    "    --use_xnnpack=true \\\n",
    "    --num_runs=100 \\\n",
    "    --num_threads=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: adb：命令找不到\n"
     ]
    }
   ],
   "source": [
    "! adb shell /data/local/tmp/benchmark_model \\\n",
    "    --graph=/data/local/tmp/pruned_model.tflite \\\n",
    "    --use_xnnpack=true \\\n",
    "    --num_runs=100 \\\n",
    "    --num_threads=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. <a id='toc1_5_'></a>[查看 Model 大小](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "models_dir = pathlib.Path(\"/tmp/models/\")\n",
    "models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "總用量 24K\n",
      "-rw-rw-r-- 1 cosmo cosmo 12K  6月 30 11:11 dense_tflite_model.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo 11K  6月 30 11:11 pruned_tflite_model.tflite\n"
     ]
    }
   ],
   "source": [
    "tflite_model_file = models_dir/\"dense_tflite_model.tflite\"\n",
    "tflite_model_file.write_bytes(dense_tflite_model)\n",
    "\n",
    "tflite_model_file = models_dir/\"pruned_tflite_model.tflite\"\n",
    "tflite_model_file.write_bytes(pruned_tflite_model)\n",
    "\n",
    "!ls -lh {models_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "# 刪除資料夾裡的 model\n",
    "!rm -rf {models_dir}/*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. <a id='toc1_6_'></a>[Conclusion 結論](#toc0_)\n",
    "\n",
    "In this tutorial, we show how one may create sparse models for faster on-device performance using the new functionality introduced by the TF MOT API and XNNPack. These sparse models are smaller and faster than their dense counterparts while retaining or even surpassing their quality.\n",
    "\n",
    "在本教程中，我們將展示如何使用TF MOT API和XNNPack引入的新功能創建稀疏模型，以提高設備上的性能。這些稀疏模型比密集模型更小，更快，同時保持甚至超過它們的質量。\n",
    "\n",
    "We encourage you to try this new capability which can be particularly important for deploying your models on device.\n",
    "我們鼓勵您嘗試此新功能，它對於在設備上部署模型尤其重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow_2.8.3__Python_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
