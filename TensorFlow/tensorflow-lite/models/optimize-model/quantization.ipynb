{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Overview](#toc1_)    \n",
    "  - 1.1. [Types of optimization](#toc1_1_)    \n",
    "  - 1.2. [Quantization](#toc1_2_)    \n",
    "- 2. [Post-training dynamic range quantization](#toc2_)    \n",
    "  - 2.1. [Overview](#toc2_1_)    \n",
    "  - 2.2. [Build an MNIST model](#toc2_2_)    \n",
    "    - 2.2.1. [Setup](#toc2_2_1_)    \n",
    "    - 2.2.2. [Train a TensorFlow model](#toc2_2_2_)    \n",
    "    - 2.2.3. [Convert to a TensorFlow Lite model](#toc2_2_3_)    \n",
    "  - 2.3. [Run the TFLite models](#toc2_3_)    \n",
    "    - 2.3.1. [Load the model into an interpreter](#toc2_3_1_)    \n",
    "    - 2.3.2. [Test the model on one image](#toc2_3_2_)    \n",
    "    - 2.3.3. [Evaluate the models](#toc2_3_3_)    \n",
    "  - 2.4. [Optimizing an existing model](#toc2_4_)    \n",
    "- 3. [Post-training float16 quantization](#toc3_)    \n",
    "  - 3.1. [Overview](#toc3_1_)    \n",
    "  - 3.2. [Build an MNIST model](#toc3_2_)    \n",
    "    - 3.2.1. [Setup](#toc3_2_1_)    \n",
    "    - 3.2.2. [Train and export the model](#toc3_2_2_)    \n",
    "    - 3.2.3. [Convert to a TensorFlow Lite model](#toc3_2_3_)    \n",
    "  - 3.3. [Run the TensorFlow Lite models](#toc3_3_)    \n",
    "    - 3.3.1. [Load the model into the interpreters](#toc3_3_1_)    \n",
    "    - 3.3.2. [Test the models on one image](#toc3_3_2_)    \n",
    "      - 3.3.2.1. [tflite_model testing](#toc3_3_2_1_)    \n",
    "      - 3.3.2.2. [Quantized tflite_model testing](#toc3_3_2_2_)    \n",
    "    - 3.3.3. [Evaluate the models](#toc3_3_3_)    \n",
    "- 4. [Post-training integer quantization](#toc4_)    \n",
    "  - 4.1. [Overview](#toc4_1_)    \n",
    "  - 4.2. [Setup](#toc4_2_)    \n",
    "  - 4.3. [Generate a TensorFlow Model](#toc4_3_)    \n",
    "  - 4.4. [Convert to a TensorFlow Lite model](#toc4_4_)    \n",
    "    - 4.4.1. [Convert using dynamic range quantization](#toc4_4_1_)    \n",
    "    - 4.4.2. [Convert using float fallback quantization](#toc4_4_2_)    \n",
    "    - 4.4.3. [Convert using integer-only quantization](#toc4_4_3_)    \n",
    "    - 4.4.4. [Save the models as files](#toc4_4_4_)    \n",
    "  - 4.5. [Run the TensorFlow Lite models](#toc4_5_)    \n",
    "    - 4.5.1. [Test the models on one image](#toc4_5_1_)    \n",
    "    - 4.5.2. [Evaluate the models on all images](#toc4_5_2_)    \n",
    "- 5. [Delete all files](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posix = Linux, nt = windows, java = java\n",
      "your os name is posix\n",
      "your os is  Linux5.15.0-73-generic\n",
      "python version is 3.9.16\n",
      "TensorFlow version is 2.8.3\n",
      "Are we using a GPU? True\n"
     ]
    }
   ],
   "source": [
    "# Check your software and os version\n",
    "import os\n",
    "print(\"posix = Linux, nt = windows, java = java\")\n",
    "print(\"your os name is\",os.name)\n",
    "\n",
    "import platform\n",
    "print(\"your os is \",platform.system()+platform.release())\n",
    "\n",
    "from platform import python_version\n",
    "print(\"python version is\",python_version())\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version is\",tf.__version__)\n",
    "\n",
    "print(\"Are we using a GPU?\",len(tf.config.experimental.list_physical_devices('GPU'))>0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "posix = Linux, nt = windows, java = java\n",
    "your os name is posix\n",
    "your os is  Linux5.15.0-73-generic\n",
    "python version is 3.9.16\n",
    "TensorFlow version is 2.8.3\n",
    "Are we using a GPU? True\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1_'></a>[Overview](#toc0_)\n",
    "\n",
    "### 1.1. <a id='toc1_1_'></a>[Types of optimization](#toc0_)\n",
    "TensorFlow Lite currently supports optimization via quantization, pruning and clustering.\n",
    "\n",
    "These are part of the [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization), which provides resources for model optimization techniques that are compatible with TensorFlow Lite.\n",
    "\n",
    "### 1.2. <a id='toc1_2_'></a>[Quantization](#toc0_)\n",
    "[Quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) works by reducing the precision of the numbers used to represent a model's parameters, which by default are 32-bit floating point numbers. This results in a smaller model size and faster computation.\n",
    "\n",
    "The following types of quantization are available in TensorFlow Lite:\n",
    "|  Technique   | Data requirements  | Size reduction  | Accuracy  | Supported hardware |\n",
    "|  ----  | ----  | ----  | ----  | ----  |\n",
    "| [Post-training float16 quantization](https://www.tensorflow.org/lite/performance/post_training_float16_quant)  | No data | Up to 50% | Insignificant accuracy loss | CPU, GPU |\n",
    "| [Post-training dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant)  | No data | Up to 75% | Smallest accuracy loss | CPU, GPU (Android) |\n",
    "| [Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant)  | Unlabelled representative sample | Up to 75% | Small accuracy loss | CPU, GPU (Android), EdgeTPU, Hexagon DSP |\n",
    "| [Quantization-aware training](http://www.tensorflow.org/model_optimization/guide/quantization/training)  | Labelled training data | Up to 75% | Smallest accuracy loss | CPU, GPU (Android), EdgeTPU, Hexagon DSP |\n",
    "\n",
    "more information: https://www.tensorflow.org/lite/performance/model_optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>[Post-training dynamic range quantization](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/post_training_quant\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/performance/post_training_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. <a id='toc2_1_'></a>[Overview](#toc0_)\n",
    "\n",
    "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports\n",
    "converting weights to 8 bit precision as part of model conversion from\n",
    "tensorflow graphdefs to TensorFlow Lite's flat buffer format. Dynamic range quantization achieves a 4x reduction in the model size. In addition, TFLite supports on the fly quantization and dequantization of activations to allow for:\n",
    "\n",
    "1.  Using quantized kernels for faster implementation when available.\n",
    "2.  Mixing of floating-point kernels with quantized kernels for different parts\n",
    "    of the graph.\n",
    "\n",
    "The activations are always stored in floating point. For ops that\n",
    "support quantized kernels, the activations are quantized to 8 bits of precision\n",
    "dynamically prior to processing and are de-quantized to float precision after\n",
    "processing. Depending on the model being converted, this can give a speedup over\n",
    "pure floating point computation.\n",
    "\n",
    "In contrast to\n",
    "[quantization aware training](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/contrib/quantize)\n",
    ", the weights are quantized post training and the activations are quantized dynamically \n",
    "at inference in this method.\n",
    "Therefore, the model weights are not retrained to compensate for quantization\n",
    "induced errors. It is important to check the accuracy of the quantized model to\n",
    "ensure that the degradation is acceptable.\n",
    "\n",
    "This tutorial trains an MNIST model from scratch, checks its accuracy in\n",
    "TensorFlow, and then converts the model into a Tensorflow Lite flatbuffer\n",
    "with dynamic range quantization. Finally, it checks the\n",
    "accuracy of the converted model and compare it to the original float model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <a id='toc2_2_'></a>[Build an MNIST model](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. <a id='toc2_2_1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. <a id='toc2_2_2_'></a>[Train a TensorFlow model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 124/1875 [>.............................] - ETA: 3s - loss: 0.9939 - accuracy: 0.7404"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2908 - accuracy: 0.9182 - val_loss: 0.1397 - val_accuracy: 0.9593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7208692f70>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, since you trained the model for just a single epoch, so it only trains to ~96% accuracy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. <a id='toc2_2_3_'></a>[Convert to a TensorFlow Lite model](#toc0_)\n",
    "\n",
    "Using the TensorFlow Lite [Converter](https://www.tensorflow.org/lite/models/convert), you can now convert the trained model into a TensorFlow Lite model.\n",
    "\n",
    "Now load the model using the `TFLiteConverter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprdperqhr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprdperqhr/assets\n",
      "2023-06-27 21:48:03.426866: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:48:03.426883: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:48:03.426987: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmprdperqhr\n",
      "2023-06-27 21:48:03.427658: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:48:03.427668: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmprdperqhr\n",
      "2023-06-27 21:48:03.429866: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:48:03.446159: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmprdperqhr\n",
      "2023-06-27 21:48:03.451536: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 24550 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write it out to a tflite file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84572"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantize the model on export, set the `optimizations` flag to optimize for size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphj9yf_bi/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphj9yf_bi/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:48:03.765625: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:48:03.765642: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:48:03.765747: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmphj9yf_bi\n",
      "2023-06-27 21:48:03.766426: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:48:03.766437: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmphj9yf_bi\n",
      "2023-06-27 21:48:03.768196: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:48:03.784346: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmphj9yf_bi\n",
      "2023-06-27 21:48:03.789681: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 23935 microseconds.\n",
      "2023-06-27 21:48:03.811463: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor sequential_3/conv2d_2/Conv2D because it has fewer than 1024 elements (108).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23808"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the resulting file, is approximately `1/4` the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "總用量 108K\n",
      "-rw-rw-r-- 1 cosmo cosmo 24K  6月 27 21:48 mnist_model_quant.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo 83K  6月 27 21:48 mnist_model.tflite\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {tflite_models_dir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. <a id='toc2_3_'></a>[Run the TFLite models](#toc0_)\n",
    "\n",
    "Run the TensorFlow Lite model using the Python TensorFlow Lite\n",
    "Interpreter.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. <a id='toc2_3_1_'></a>[Load the model into an interpreter](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. <a id='toc2_3_2_'></a>[Test the model on one image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "interpreter.set_tensor(input_index, test_image)\n",
    "interpreter.invoke()\n",
    "predictions = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. <a id='toc2_3_3_'></a>[Evaluate the models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for test_image in test_images:\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  accurate_count = 0\n",
    "  for index in range(len(prediction_digits)):\n",
    "    if prediction_digits[index] == test_labels[index]:\n",
    "      accurate_count += 1\n",
    "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9593\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(interpreter))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the evaluation on the dynamic range quantized model to obtain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(interpreter_quant))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the compressed model has no difference in the accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. <a id='toc2_4_'></a>[Optimizing an existing model](#toc0_)\n",
    "\n",
    "Resnets with pre-activation layers (Resnet-v2) are widely used for vision applications.\n",
    "  Pre-trained frozen graph for resnet-v2-101 is available on\n",
    "  [Tensorflow Hub](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4).\n",
    "\n",
    "You can convert the frozen graph to a TensorFLow Lite flatbuffer with quantization by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "resnet_v2_101 = tf.keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(resnet_v2_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvp1olxjs/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvp1olxjs/assets\n",
      "2023-06-27 21:48:13.320437: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:48:13.320458: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:48:13.320580: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpvp1olxjs\n",
      "2023-06-27 21:48:13.336665: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:48:13.336686: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpvp1olxjs\n",
      "2023-06-27 21:48:13.396879: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:48:13.858571: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpvp1olxjs\n",
      "2023-06-27 21:48:14.030975: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 710395 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "178426644"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to TF Lite without quantization\n",
    "resnet_tflite_file = tflite_models_dir/\"resnet_v2_101.tflite\"\n",
    "resnet_tflite_file.write_bytes(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcsds19wm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcsds19wm/assets\n",
      "2023-06-27 21:48:22.451044: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:48:22.451067: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:48:22.451190: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpcsds19wm\n",
      "2023-06-27 21:48:22.470733: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:48:22.470753: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpcsds19wm\n",
      "2023-06-27 21:48:22.542699: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:48:22.980242: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpcsds19wm\n",
      "2023-06-27 21:48:23.162326: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 711136 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46169720"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to TF Lite with quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "resnet_quantized_tflite_file = tflite_models_dir/\"resnet_v2_101_quantized.tflite\"\n",
    "resnet_quantized_tflite_file.write_bytes(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "-rw-rw-r-- 1 cosmo cosmo  24K  6月 27 21:48 /tmp/mnist_tflite_models/mnist_model_quant.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  83K  6月 27 21:48 /tmp/mnist_tflite_models/mnist_model.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  45M  6月 27 21:48 /tmp/mnist_tflite_models/resnet_v2_101_quantized.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo 171M  6月 27 21:48 /tmp/mnist_tflite_models/resnet_v2_101.tflite\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {tflite_models_dir}/*.tflite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model size reduces from 171 MB to 43 MB.\n",
    "The accuracy of this model on imagenet can be evaluated using the scripts provided for [TFLite accuracy measurement](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/imagenet_image_classification).\n",
    "\n",
    "The optimized model top-1 accuracy is 76.8, the same as the floating point model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>[Post-training float16 quantization](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/post_training_float16_quant\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_float16_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_float16_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/performance/post_training_float16_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. <a id='toc3_1_'></a>[Overview](#toc0_)\n",
    "\n",
    "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports\n",
    "converting weights to 16-bit floating point values during model conversion from TensorFlow to TensorFlow Lite's flat buffer format. This results in a 2x reduction in model size. Some hardware, like GPUs, can compute natively in this reduced precision arithmetic, realizing a speedup over traditional floating point execution. The Tensorflow Lite GPU delegate can be configured to run in this way. However, a model converted to float16 weights can still run on the CPU without additional modification: the float16 weights are upsampled to float32 prior to the first inference. This permits a significant reduction in model size in exchange for a minimal impacts to latency and accuracy.\n",
    "\n",
    "In this tutorial, you train an MNIST model from scratch, check its accuracy in TensorFlow, and then convert the model into a Tensorflow Lite flatbuffer\n",
    "with float16 quantization. Finally, check the accuracy of the converted model and compare it to the original float32 model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. <a id='toc3_2_'></a>[Build an MNIST model](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. <a id='toc3_2_1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. <a id='toc3_2_2_'></a>[Train and export the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3016 - accuracy: 0.9137 - val_loss: 0.1584 - val_accuracy: 0.9546\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1366 - accuracy: 0.9607 - val_loss: 0.1027 - val_accuracy: 0.9700\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0987 - accuracy: 0.9715 - val_loss: 0.0839 - val_accuracy: 0.9735\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0789 - accuracy: 0.9771 - val_loss: 0.0713 - val_accuracy: 0.9778\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0659 - accuracy: 0.9808 - val_loss: 0.0634 - val_accuracy: 0.9790\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0565 - accuracy: 0.9831 - val_loss: 0.0622 - val_accuracy: 0.9793\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0500 - accuracy: 0.9850 - val_loss: 0.0591 - val_accuracy: 0.9803\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0447 - accuracy: 0.9871 - val_loss: 0.0598 - val_accuracy: 0.9801\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0406 - accuracy: 0.9878 - val_loss: 0.0608 - val_accuracy: 0.9803\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0363 - accuracy: 0.9894 - val_loss: 0.0557 - val_accuracy: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72149e5070>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=10,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, you trained the model for just a single epoch, so it only trains to ~96% accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. <a id='toc3_2_3_'></a>[Convert to a TensorFlow Lite model](#toc0_)\n",
    "\n",
    "Using the TensorFlow Lite [Converter](https://www.tensorflow.org/lite/models/convert), you can now convert the trained model into a TensorFlow Lite model.\n",
    "\n",
    "Now load the model using the `TFLiteConverter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp14b5z2md/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp14b5z2md/assets\n",
      "2023-06-27 21:49:17.967455: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:49:17.967471: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:49:17.967578: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp14b5z2md\n",
      "2023-06-27 21:49:17.968311: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:49:17.968320: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmp14b5z2md\n",
      "2023-06-27 21:49:17.970784: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:49:17.988847: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmp14b5z2md\n",
      "2023-06-27 21:49:17.995087: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 27510 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write it out to a `.tflite` file:\n",
    "\n",
    "这段代码使用 Python 的 pathlib 模块创建了一个名为 \"/tmp/mnist_tflite_models/\" 的文件夹路径，并将其保存在一个名为 tflite_models_dir 的变量中。\n",
    "\n",
    "接下来，调用 mkdir() 方法，将在本地文件系统上创建一个名为 \"/tmp/mnist_tflite_models/\" 的文件夹。如果文件夹已经存在，则不会抛出异常，因为 exist_ok=True 参数告诉 Python 在文件夹已经存在时不要引发异常。parents=True 参数告诉 Python 创建整个目录树，即如果上级目录不存在，也要一并创建。\n",
    "\n",
    "这样，我们就可以使用 tflite_models_dir 变量引用 \"/tmp/mnist_tflite_models/\" 文件夹路径，以便在后续代码中使用该文件夹来存储 TFLite 模型文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/mnist_tflite_models')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "tflite_models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84572"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instead quantize the model to float16 on export, first set the `optimizations` flag to use default optimizations. Then specify that float16 is the supported type on the target platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用默认的优化方式对模型进行优化。\n",
    "# 这些优化方式可以提高模型的性能和准确度，并减小模型文件的大小。\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] \n",
    "\n",
    "# 将 converter 对象的 target_spec.supported_types 属性设置为 [tf.float16]，\n",
    "# 表示在模型量化时使用 float16 数据类型。这将有助于减小模型的文件大小，\n",
    "# 并提高模型在支持 float16 的硬件上的运行速度。\n",
    "converter.target_spec.supported_types = [tf.float16] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, convert the model like usual. Note, by default the converted model will still use float input and outputs for invocation convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpar3ieijw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpar3ieijw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 21:49:18.326764: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:49:18.326780: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:49:18.326890: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpar3ieijw\n",
      "2023-06-27 21:49:18.327714: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:49:18.327724: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpar3ieijw\n",
      "2023-06-27 21:49:18.329915: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:49:18.346722: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpar3ieijw\n",
      "2023-06-27 21:49:18.352582: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 25692 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44268"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_fp16_model = converter.convert()\n",
    "tflite_model_fp16_file = tflite_models_dir/\"mnist_model_quant_f16.tflite\"\n",
    "tflite_model_fp16_file.write_bytes(tflite_fp16_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the resulting file is approximately `1/2` the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "總用量 215M\n",
      "-rw-rw-r-- 1 cosmo cosmo  44K  6月 27 21:49 mnist_model_quant_f16.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  24K  6月 27 21:48 mnist_model_quant.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  83K  6月 27 21:49 mnist_model.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  45M  6月 27 21:48 resnet_v2_101_quantized.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo 171M  6月 27 21:48 resnet_v2_101.tflite\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {tflite_models_dir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. <a id='toc3_3_'></a>[Run the TensorFlow Lite models](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the TensorFlow Lite model using the Python TensorFlow Lite Interpreter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. <a id='toc3_3_1_'></a>[Load the model into the interpreters](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_fp16 = tf.lite.Interpreter(model_path=str(tflite_model_fp16_file))\n",
    "interpreter_fp16.allocate_tensors()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. <a id='toc3_3_2_'></a>[Test the models on one image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2.1. <a id='toc3_3_2_1_'></a>[tflite_model testing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f721bae44f0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "plt.imshow(test_image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 17)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "input_index, output_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.997165  ,  -8.649213  ,  -4.5491643 ,   0.47341782,\n",
       "         -8.491926  , -11.217087  , -30.570215  ,  10.592542  ,\n",
       "         -5.8169765 ,  -2.5953789 ]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.set_tensor(input_index, test_image)\n",
    "interpreter.invoke()\n",
    "predictions = interpreter.get_tensor(output_index)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2.2. <a id='toc3_3_2_2_'></a>[Quantized tflite_model testing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "\n",
    "input_index = interpreter_fp16.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter_fp16.get_output_details()[0][\"index\"]\n",
    "\n",
    "interpreter_fp16.set_tensor(input_index, test_image)\n",
    "interpreter_fp16.invoke()\n",
    "predictions = interpreter_fp16.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. <a id='toc3_3_3_'></a>[Evaluate the models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for test_image in test_images:\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  accurate_count = 0\n",
    "  for index in range(len(prediction_digits)):\n",
    "    if prediction_digits[index] == test_labels[index]:\n",
    "      accurate_count += 1\n",
    "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9835\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(interpreter))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the evaluation on the float16 quantized model to obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9835\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Colab runs on server CPUs. At the time of writing this, TensorFlow Lite\n",
    "# doesn't have super optimized server CPU kernels. For this reason this may be\n",
    "# slower than the above float interpreter. But for mobile CPUs, considerable\n",
    "# speedup can be observed.\n",
    "print(evaluate_model(interpreter_fp16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you have quantized a model to float16 with no difference in the accuracy.\n",
    "\n",
    "It's also possible to evaluate the fp16 quantized model on the GPU. To perform all arithmetic with the reduced precision values, be sure to create the `TfLiteGPUDelegateOptions` struct in your app and set `precision_loss_allowed` to `1`, like this:\n",
    "\n",
    "```\n",
    "//Prepare GPU delegate.\n",
    "const TfLiteGpuDelegateOptions options = {\n",
    "  .metadata = NULL,\n",
    "  .compile_options = {\n",
    "    .precision_loss_allowed = 1,  // FP16\n",
    "    .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,\n",
    "    .dynamic_batch_enabled = 0,   // Not fully functional yet\n",
    "  },\n",
    "};\n",
    "```\n",
    "\n",
    "Detailed documentation on the TFLite GPU delegate and how to use it in your application can be found [here](https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>[Post-training integer quantization](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/post_training_integer_quant\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. <a id='toc4_1_'></a>[Overview](#toc0_)\n",
    "\n",
    "Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers. This results in a smaller model and increased inferencing speed, which is valuable for low-power devices such as [microcontrollers](https://www.tensorflow.org/lite/microcontrollers). This data format is also required by integer-only accelerators such as the [Edge TPU](https://coral.ai/).\n",
    "\n",
    "In this tutorial, you'll train an MNIST model from scratch, convert it into a Tensorflow Lite file, and quantize it using [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization). Finally, you'll check the accuracy of the converted model and compare it to the original float model.\n",
    "\n",
    "You actually have several options as to how much you want to quantize a model. In this tutorial, you'll perform \"full integer quantization,\" which converts all weights and activation outputs into 8-bit integer data—whereas other strategies may leave some amount of data in floating-point.\n",
    "\n",
    "To learn more about the various quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. <a id='toc4_2_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantize both the input and output tensors, we need to use APIs added in TensorFlow 2.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.8.3\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. <a id='toc4_3_'></a>[Generate a TensorFlow Model](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a simple model to classify numbers from the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist).\n",
    "\n",
    "This training won't take long because you're training the model for just a 5 epochs, which trains to about ~98% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2689 - accuracy: 0.9249 - val_loss: 0.1138 - val_accuracy: 0.9669\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1070 - accuracy: 0.9688 - val_loss: 0.0860 - val_accuracy: 0.9727\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0793 - accuracy: 0.9765 - val_loss: 0.0722 - val_accuracy: 0.9756\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0650 - accuracy: 0.9815 - val_loss: 0.0625 - val_accuracy: 0.9781\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0559 - accuracy: 0.9836 - val_loss: 0.0589 - val_accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f720ab823d0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. <a id='toc4_4_'></a>[Convert to a TensorFlow Lite model](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can convert the trained model to TensorFlow Lite format using the TensorFlow Lite [Converter](https://www.tensorflow.org/lite/models/convert), and apply varying degrees of quantization.\n",
    "\n",
    "Beware that some versions of quantization leave some of the data in float format. So the following sections show each option with increasing amounts of quantization, until we get a model that's entirely int8 or uint8 data. (Notice we duplicate some code in each section so you can see all the quantization steps for each option.)\n",
    "\n",
    "First, here's a converted model with no quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa2awgmj0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa2awgmj0/assets\n",
      "2023-06-27 21:49:45.161060: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:49:45.161076: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:49:45.161177: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpa2awgmj0\n",
      "2023-06-27 21:49:45.161849: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:49:45.161860: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpa2awgmj0\n",
      "2023-06-27 21:49:45.164081: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:49:45.180205: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpa2awgmj0\n",
      "2023-06-27 21:49:45.185744: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 24567 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. <a id='toc4_4_1_'></a>[Convert using dynamic range quantization](#toc0_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's enable the default `optimizations` flag to quantize all fixed parameters (such as weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpisy4l4bn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpisy4l4bn/assets\n",
      "2023-06-27 21:49:45.495970: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:49:45.495986: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:49:45.496090: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpisy4l4bn\n",
      "2023-06-27 21:49:45.496715: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:49:45.496724: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpisy4l4bn\n",
      "2023-06-27 21:49:45.498778: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:49:45.514845: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpisy4l4bn\n",
      "2023-06-27 21:49:45.520052: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 23962 microseconds.\n",
      "2023-06-27 21:49:45.541957: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor sequential_6/conv2d_4/Conv2D because it has fewer than 1024 elements (108).\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now a bit smaller with quantized weights, but other variable data is still in float format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2. <a id='toc4_4_2_'></a>[Convert using float fallback quantization](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantize the variable data (such as model input/output and intermediates between layers), you need to provide a [`RepresentativeDataset`](https://www.tensorflow.org/api_docs/python/tf/lite/RepresentativeDataset). This is a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.)\n",
    "To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqfkhqisr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqfkhqisr/assets\n",
      "/home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-06-27 21:49:45.825754: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:49:45.825769: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:49:45.825871: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpqfkhqisr\n",
      "2023-06-27 21:49:45.826476: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:49:45.826485: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpqfkhqisr\n",
      "2023-06-27 21:49:45.828424: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:49:45.843975: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpqfkhqisr\n",
      "2023-06-27 21:49:45.849273: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 23402 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all weights and variable data are quantized, and the model is significantly smaller compared to the original TensorFlow Lite model.\n",
    "\n",
    "However, to maintain compatibility with applications that traditionally use float model input and output tensors, the TensorFlow Lite Converter leaves the model input and output tensors in float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's usually good for compatibility, but it won't be compatible with devices that perform only integer-based operations, such as the Edge TPU.\n",
    "\n",
    "Additionally, the above process may leave an operation in float format if TensorFlow Lite doesn't include a quantized implementation for that operation. This strategy allows conversion to complete so you have a smaller and more efficient model, but again, it won't be compatible with integer-only hardware. (All ops in this MNIST model have a quantized implementation.)\n",
    "\n",
    "So to ensure an end-to-end integer-only model, you need a couple more parameters..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. <a id='toc4_4_3_'></a>[Convert using integer-only quantization](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantize the input and output tensors, and make the converter throw an error if it encounters an operation it cannot quantize, convert the model again with some additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2ja094gu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2ja094gu/assets\n",
      "/home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-06-27 21:49:46.358066: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-27 21:49:46.358082: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-27 21:49:46.358185: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp2ja094gu\n",
      "2023-06-27 21:49:46.358772: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-27 21:49:46.358781: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmp2ja094gu\n",
      "2023-06-27 21:49:46.360690: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-27 21:49:46.376313: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmp2ja094gu\n",
      "2023-06-27 21:49:46.381485: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 23300 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal quantization remains the same as above, but you can see the input and output tensors are now integer format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have an integer quantized model that uses integer data for the model's input and output tensors, so it's compatible with integer-only hardware such as the [Edge TPU](https://coral.ai)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4. <a id='toc4_4_4_'></a>[Save the models as files](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need a `.tflite` file to deploy your model on other devices. So let's save the converted models to files and then load them when we run inferences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24368"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant_int.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "總用量 215M\n",
      "-rw-rw-r-- 1 cosmo cosmo  44K  6月 27 21:49 mnist_model_quant_f16.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  24K  6月 27 21:49 mnist_model_quant_int.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  24K  6月 27 21:48 mnist_model_quant.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  83K  6月 27 21:49 mnist_model.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo  45M  6月 27 21:48 resnet_v2_101_quantized.tflite\n",
      "-rw-rw-r-- 1 cosmo cosmo 171M  6月 27 21:48 resnet_v2_101.tflite\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {tflite_models_dir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. <a id='toc4_5_'></a>[Run the TensorFlow Lite models](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run inferences using the TensorFlow Lite [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) to compare the model accuracies.\n",
    "\n",
    "First, we need a function that runs inference with a given model and images, and then returns the predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global test_images\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_images[test_image_index]\n",
    "    test_label = test_labels[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1. <a id='toc4_5_1_'></a>[Test the models on one image](#toc0_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compare the performance of the float model and quantized model:\n",
    "+ `tflite_model_file` is the original TensorFlow Lite model with floating-point data.\n",
    "+ `tflite_model_quant_file` is the last model we converted using integer-only quantization (it uses uint8 data for input and output).\n",
    "\n",
    "Let's create another function to print our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Change this to test a different image\n",
    "test_image_index = 1\n",
    "\n",
    "## Helper function to test the models on one image\n",
    "def test_model(tflite_file, test_image_index, model_type):\n",
    "  global test_labels\n",
    "\n",
    "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
    "\n",
    "  plt.imshow(test_images[test_image_index])\n",
    "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
    "  _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))\n",
    "  plt.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the float model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2. <a id='toc4_5_2_'></a>[Evaluate the models on all images](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run both models using all the test images we loaded at the beginning of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global test_images\n",
    "  global test_labels\n",
    "\n",
    "  test_image_indices = range(test_images.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the float model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 98.0600% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 98.0300% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you now have an integer quantized a model with almost no difference in the accuracy, compared to the float model.\n",
    "\n",
    "To learn more about other quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>[Delete all files](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cosmo/anaconda3/envs/TensorFlow_2.8.3__Python_3.9/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf {tflite_models_dir}/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow_2.8.3__Python_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
