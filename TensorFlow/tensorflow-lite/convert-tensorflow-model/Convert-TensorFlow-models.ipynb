{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion workflow\n",
    "The diagram below illustrations the high-level workflow for converting your model:\n",
    "\n",
    "![convert](../assets/convert.png)\n",
    "\n",
    "You can convert your model using one of the following options:\n",
    "1. [Python API](https://www.tensorflow.org/lite/models/convert/convert_models#python_api) (recommended): This allows you to integrate the conversion into your development pipeline, apply optimizations, add metadata and many other tasks that simplify the conversion process.\n",
    "2. [Command line](https://www.tensorflow.org/lite/models/convert/convert_models#cmdline): This only supports basic model conversion.\n",
    "\n",
    "> NOTE: In case you encounter any issues during model conversion, create a [GitHub issue](https://github.com/tensorflow/tensorflow/issues/new?template=60-tflite-converter-issue.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python API\n",
    "\n",
    "Helper code: To learn more about the TensorFlow Lite converter API, run print(help(tf.lite.TFLiteConverter)).\n",
    "\n",
    "使用 轉換 TensorFlow 模型 [tf.lite.TFLiteConverter](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter)。TensorFlow 模型使用 SavedModel 格式存儲，並使用高級tf.keras.*API（Keras 模型）或低級tf.*API（從中生成具體函數）生成。因此，您有以下三個選項（示例位於接下來的幾節中）：\n",
    "\n",
    "* [tf.lite.TFLiteConverter.from_saved_model()](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_saved_model)（推薦）：轉換一個SavedModel。\n",
    "* [tf.lite.TFLiteConverter.from_keras_model()](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_keras_model)：轉換 Keras模型。\n",
    "* [tf.lite.TFLiteConverter.from_concrete_functions()](https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_concrete_functions): 轉換 具體函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TFLiteConverterV2 in module tensorflow.lite.python.lite:\n",
      "\n",
      "class TFLiteConverterV2(TFLiteFrozenGraphConverterV2)\n",
      " |  TFLiteConverterV2(funcs, trackable_obj=None)\n",
      " |  \n",
      " |  Converts a TensorFlow model into TensorFlow Lite model.\n",
      " |  \n",
      " |  Attributes:\n",
      " |    optimizations: Experimental flag, subject to change. Set of optimizations to\n",
      " |      apply. e.g {tf.lite.Optimize.DEFAULT}. (default None, must be None or a\n",
      " |      set of values of type `tf.lite.Optimize`)\n",
      " |    representative_dataset: A generator function used for integer quantization\n",
      " |      where each generated sample has the same order, type and shape as the\n",
      " |      inputs to the model. Usually, this is a small subset of a few hundred\n",
      " |      samples randomly chosen, in no particular order, from the training or\n",
      " |      evaluation dataset. This is an optional attribute, but required for full\n",
      " |      integer quantization, i.e, if `tf.int8` is the only supported type in\n",
      " |      `target_spec.supported_types`. Refer to `tf.lite.RepresentativeDataset`.\n",
      " |      (default None)\n",
      " |    target_spec: Experimental flag, subject to change. Specifications of target\n",
      " |      device, including supported ops set, supported types and a set of user's\n",
      " |      defined TensorFlow operators required in the TensorFlow Lite runtime.\n",
      " |      Refer to `tf.lite.TargetSpec`.\n",
      " |    inference_input_type: Data type of the input layer. Note that integer types\n",
      " |      (tf.int8 and tf.uint8) are currently only supported for post training\n",
      " |      integer quantization and quantization aware training. (default tf.float32,\n",
      " |      must be in {tf.float32, tf.int8, tf.uint8})\n",
      " |    inference_output_type: Data type of the output layer. Note that integer\n",
      " |      types (tf.int8 and tf.uint8) are currently only supported for post\n",
      " |      training integer quantization and quantization aware training. (default\n",
      " |      tf.float32, must be in {tf.float32, tf.int8, tf.uint8})\n",
      " |    allow_custom_ops: Boolean indicating whether to allow custom operations.\n",
      " |      When False, any unknown operation is an error. When True, custom ops are\n",
      " |      created for any op that is unknown. The developer needs to provide these\n",
      " |      to the TensorFlow Lite runtime with a custom resolver. (default False)\n",
      " |    experimental_new_converter: Experimental flag, subject to change. Enables\n",
      " |      MLIR-based conversion. (default True)\n",
      " |    experimental_new_quantizer: Experimental flag, subject to change. Enables\n",
      " |      MLIR-based quantization conversion instead of Flatbuffer-based conversion.\n",
      " |      (default True)\n",
      " |    experimental_enable_resource_variables: Experimental flag, subject to\n",
      " |      change. Enables resource variables to be converted by this converter. This\n",
      " |      is only allowed if from_saved_model interface is used. (default False)\n",
      " |  \n",
      " |  Example usage:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Converting a SavedModel to a TensorFlow Lite model.\n",
      " |    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
      " |    tflite_model = converter.convert()\n",
      " |  \n",
      " |  # Converting a tf.Keras model to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
      " |  tflite_model = converter.convert()\n",
      " |  \n",
      " |  # Converting ConcreteFunctions to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)\n",
      " |  tflite_model = converter.convert()\n",
      " |  \n",
      " |  # Converting a Jax model to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.experimental_from_jax([func], [[\n",
      " |      ('input1', input1), ('input2', input2)])\n",
      " |  tflite_model = converter.convert()\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TFLiteConverterV2\n",
      " |      TFLiteFrozenGraphConverterV2\n",
      " |      TFLiteConverterBaseV2\n",
      " |      TFLiteConverterBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, funcs, trackable_obj=None)\n",
      " |      Constructor for TFLiteConverter.\n",
      " |      \n",
      " |      Args:\n",
      " |        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n",
      " |          duplicate elements.\n",
      " |        trackable_obj: tf.AutoTrackable object associated with `funcs`. A\n",
      " |          reference to this object needs to be maintained so that Variables do not\n",
      " |          get garbage collected since functions have a weak reference to\n",
      " |          Variables. This is only required when the tf.AutoTrackable object is not\n",
      " |          maintained by the user (e.g. `from_saved_model`).\n",
      " |  \n",
      " |  convert(self)\n",
      " |      Converts a TensorFlow GraphDef based on instance variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The converted data in serialized format.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError:\n",
      " |          No concrete functions is specified.\n",
      " |          Multiple concrete functions are specified.\n",
      " |          Input shape is not specified.\n",
      " |          Invalid quantization parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  experimental_from_jax(serving_funcs, inputs) from builtins.type\n",
      " |      Creates a TFLiteConverter object from a Jax model with its inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        serving_funcs: A array of Jax functions with all the weights applied\n",
      " |          already.\n",
      " |        inputs: A array of Jax input placeholders tuples list, e.g.,\n",
      " |          jnp.zeros(INPUT_SHAPE). Each tuple list should correspond with the\n",
      " |          serving function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |  \n",
      " |  from_concrete_functions(funcs, trackable_obj=None) from builtins.type\n",
      " |      Creates a TFLiteConverter object from ConcreteFunctions.\n",
      " |      \n",
      " |      Args:\n",
      " |        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n",
      " |          duplicate elements. Currently converter can only convert a single\n",
      " |          ConcreteFunction. Converting multiple functions is under development.\n",
      " |        trackable_obj:   An `AutoTrackable` object (typically `tf.module`)\n",
      " |          associated with `funcs`. A reference to this object needs to be\n",
      " |          maintained so that Variables do not get garbage collected since\n",
      " |          functions have a weak reference to Variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |      \n",
      " |      Raises:\n",
      " |        Invalid input type.\n",
      " |  \n",
      " |  from_keras_model(model) from builtins.type\n",
      " |      Creates a TFLiteConverter object from a Keras model.\n",
      " |      \n",
      " |      Args:\n",
      " |        model: tf.Keras.Model\n",
      " |      \n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |  \n",
      " |  from_saved_model(saved_model_dir, signature_keys=None, tags=None) from builtins.type\n",
      " |      Creates a TFLiteConverter object from a SavedModel directory.\n",
      " |      \n",
      " |      Args:\n",
      " |        saved_model_dir: SavedModel directory to convert.\n",
      " |        signature_keys: List of keys identifying SignatureDef containing inputs\n",
      " |          and outputs. Elements should not be duplicated. By default the\n",
      " |          `signatures` attribute of the MetaGraphdef is used. (default\n",
      " |          saved_model.signatures)\n",
      " |        tags: Set of tags identifying the MetaGraphDef within the SavedModel to\n",
      " |          analyze. All tags in the tag set must be present. (default\n",
      " |          {tf.saved_model.SERVING} or {'serve'})\n",
      " |      \n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |      \n",
      " |      Raises:\n",
      " |        Invalid signature keys.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from TFLiteConverterBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(help(tf.lite.TFLiteConverter))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a SavedModel (recommended) \n",
    "The following example shows how to convert a [SavedModel](https://www.tensorflow.org/guide/saved_model) into a TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: saved_model_dir\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Convert the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlite\u001b[39m.\u001b[39;49mTFLiteConverter\u001b[39m.\u001b[39;49mfrom_saved_model(\u001b[39m\"\u001b[39;49m\u001b[39msaved_model_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# path to the SavedModel directory\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mconvert()\n\u001b[0;32m      7\u001b[0m \u001b[39m# Save the model.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chiou\\anaconda3\\envs\\tensorflow_2.8.3__python_3.9\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1648\u001b[0m, in \u001b[0;36mTFLiteConverterV2.from_saved_model\u001b[1;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[0;32m   1645\u001b[0m   tags \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([_tag_constants\u001b[39m.\u001b[39mSERVING])\n\u001b[0;32m   1647\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39meager_mode():\n\u001b[1;32m-> 1648\u001b[0m   saved_model \u001b[39m=\u001b[39m _load(saved_model_dir, tags)\n\u001b[0;32m   1649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signature_keys:\n\u001b[0;32m   1650\u001b[0m   signature_keys \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39msignatures\n",
      "File \u001b[1;32mc:\\Users\\chiou\\anaconda3\\envs\\tensorflow_2.8.3__python_3.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:936\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msaved_model.load\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msaved_model.load_v2\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(export_dir, tags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    847\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Load a SavedModel from `export_dir`.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \n\u001b[0;32m    849\u001b[0m \u001b[39m  Signatures associated with the SavedModel are available as functions:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39m    ValueError: If `tags` don't match a MetaGraph in the SavedModel.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 936\u001b[0m   result \u001b[39m=\u001b[39m load_internal(export_dir, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    937\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\chiou\\anaconda3\\envs\\tensorflow_2.8.3__python_3.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:949\u001b[0m, in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tags, \u001b[39mset\u001b[39m):\n\u001b[0;32m    945\u001b[0m   \u001b[39m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m   tags \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mflatten(tags)\n\u001b[0;32m    948\u001b[0m saved_model_proto, debug_info \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 949\u001b[0m     loader_impl\u001b[39m.\u001b[39;49mparse_saved_model_with_debug_info(export_dir))\n\u001b[0;32m    951\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(saved_model_proto\u001b[39m.\u001b[39mmeta_graphs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     saved_model_proto\u001b[39m.\u001b[39mmeta_graphs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mHasField(\u001b[39m\"\u001b[39m\u001b[39mobject_graph_def\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    953\u001b[0m   metrics\u001b[39m.\u001b[39mIncrementReadApi(_LOAD_V2_LABEL)\n",
      "File \u001b[1;32mc:\\Users\\chiou\\anaconda3\\envs\\tensorflow_2.8.3__python_3.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:57\u001b[0m, in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_saved_model_with_debug_info\u001b[39m(export_dir):\n\u001b[0;32m     45\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m    parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m   saved_model \u001b[39m=\u001b[39m parse_saved_model(export_dir)\n\u001b[0;32m     59\u001b[0m   debug_info_path \u001b[39m=\u001b[39m file_io\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     60\u001b[0m       saved_model_utils\u001b[39m.\u001b[39mget_debug_dir(export_dir),\n\u001b[0;32m     61\u001b[0m       constants\u001b[39m.\u001b[39mDEBUG_INFO_FILENAME_PB)\n\u001b[0;32m     62\u001b[0m   debug_info \u001b[39m=\u001b[39m graph_debug_info_pb2\u001b[39m.\u001b[39mGraphDebugInfo()\n",
      "File \u001b[1;32mc:\\Users\\chiou\\anaconda3\\envs\\tensorflow_2.8.3__python_3.9\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:115\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot parse file \u001b[39m\u001b[39m{\u001b[39;00mpath_to_pbtxt\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSavedModel file does not exist at: \u001b[39m\u001b[39m{\u001b[39;00mexport_dir\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msep\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m{\u001b[39;00mconstants\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mconstants\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PB\u001b[39m}\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: saved_model_dir\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model_dir\") # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a Keras model\n",
    "The following example shows how to convert a [Keras](https://www.tensorflow.org/guide/keras/overview) model into a TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 934ms/step - loss: 3.4129\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.2920\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.1562\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0246\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8965\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\chiou\\AppData\\Local\\Temp\\tmp3u_7o_lv\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a model using high-level tf.keras.* APIs\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=[1]),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model\n",
    "model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5) # train the model\n",
    "# (to generate a SavedModel) tf.saved_model.save(model, \"saved_model_keras_dir\")\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert concrete functions\n",
    "The following example shows how to convert [concrete functions](https://www.tensorflow.org/guide/intro_to_graphs) into a TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\chiou\\AppData\\Local\\Temp\\tmpttcokrup\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a model using low-level tf.* APIs\n",
    "class Squared(tf.Module):\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\n",
    "  def __call__(self, x):\n",
    "    return tf.square(x)\n",
    "model = Squared()\n",
    "# (ro run your model) result = Squared(5.0) # This prints \"25.0\"\n",
    "# (to generate a SavedModel) tf.saved_model.save(model, \"saved_model_tf_dir\")\n",
    "concrete_func = model.__call__.get_concrete_function()\n",
    "\n",
    "# Convert the model.\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],\n",
    "                                                            model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other features\n",
    "* Apply [optimizations](https://www.tensorflow.org/lite/performance/model_optimization). A common optimization used is [post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization), which can further reduce your model latency and size with minimal loss in accuracy.\n",
    "* Add [metadata](https://www.tensorflow.org/lite/models/convert/metadata), which makes it easier to create platform specific wrapper code when deploying models on devices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion errors\n",
    "\n",
    "如果想了解轉換錯誤以及解決方法，可以參考[轉換錯誤](https://www.tensorflow.org/lite/models/convert/convert_models#conversion_errors)\n",
    "\n",
    "**1**\n",
    "**錯誤**：Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: <a href=\"https://www.tensorflow.org/lite/guide/ops_select\">https://www.tensorflow.org/lite/guide/ops_select</a> TF Select ops: ..., .., ...\n",
    "\n",
    "**解決方案**：出現此錯誤是因為您的模型具有沒有相應 TFLite 實現的 TF 操作。您可以通過 [使用 TFLite 模型中的 TF 操作](https://www.tensorflow.org/lite/guide/ops_select)來解決此問題 （推薦）。如[果您只想生成帶有 TFLite 操作的模型，您可以在Github 問題 #21526](https://github.com/tensorflow/tensorflow/issues/21526)中添加對缺少的 TFLite 操作的請求 （如果您的請求尚未被提及，請發表評論）或 自己[創建 TFLite 操作](https://www.tensorflow.org/lite/guide/ops_custom#create_and_register_the_operator)。\n",
    "\n",
    "**2**\n",
    "錯誤：.. is neither a custom op nor a flex op\n",
    "解決方案：如果這個 TF op 是：\n",
    "* Supported in TF: The error occurs because the TF op is missing from the [allowlist](https://www.tensorflow.org/lite/guide/op_select_allowlist) (an exhaustive list of TF ops supported by TFLite). You can resolve this as follows:\n",
    "* 1. [Add missing ops to the allowlist.](https://www.tensorflow.org/lite/guide/op_select_allowlist#add_tensorflow_core_operators_to_the_allowed_list)\n",
    "* 2. [Convert the TF model to a TFLite model and run inference.](https://www.tensorflow.org/lite/guide/ops_select)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select TensorFlow operators\n",
    "## Convert a model\n",
    "The following example shows how to generate a TensorFlow Lite model with select TensorFlow ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_2.8.3__python_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
